[2026-01-15T02:27:53.319+0000] {taskinstance.py:1956} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nyc_taxi_pipeline.process manual__2026-01-15T02:25:36.535179+00:00 [queued]>
[2026-01-15T02:27:53.325+0000] {taskinstance.py:1956} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nyc_taxi_pipeline.process manual__2026-01-15T02:25:36.535179+00:00 [queued]>
[2026-01-15T02:27:53.325+0000] {taskinstance.py:2170} INFO - Starting attempt 1 of 1
[2026-01-15T02:27:53.334+0000] {taskinstance.py:2191} INFO - Executing <Task(DockerOperator): process> on 2026-01-15 02:25:36.535179+00:00
[2026-01-15T02:27:53.339+0000] {standard_task_runner.py:60} INFO - Started process 212 to run task
[2026-01-15T02:27:53.342+0000] {standard_task_runner.py:87} INFO - Running: ['***', 'tasks', 'run', 'nyc_taxi_pipeline', 'process', 'manual__2026-01-15T02:25:36.535179+00:00', '--job-id', '8', '--raw', '--subdir', 'DAGS_FOLDER/nyc_taxi_pipeline.py', '--cfg-path', '/tmp/tmpytoc0n3t']
[2026-01-15T02:27:53.344+0000] {standard_task_runner.py:88} INFO - Job 8: Subtask process
[2026-01-15T02:27:53.383+0000] {task_command.py:423} INFO - Running <TaskInstance: nyc_taxi_pipeline.process manual__2026-01-15T02:25:36.535179+00:00 [running]> on host dc390c2f2007
[2026-01-15T02:27:53.445+0000] {taskinstance.py:2480} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='nyc_taxi_pipeline' AIRFLOW_CTX_TASK_ID='process' AIRFLOW_CTX_EXECUTION_DATE='2026-01-15T02:25:36.535179+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2026-01-15T02:25:36.535179+00:00'
[2026-01-15T02:27:53.460+0000] {docker.py:359} INFO - Starting docker container from image nyc_processing:latest
[2026-01-15T02:27:53.463+0000] {docker.py:367} WARNING - Using remote engine or docker-in-docker and mounting temporary volume from host is not supported. Falling back to `mount_tmp_dir=False` mode. You can set `mount_tmp_dir` parameter to False to disable mounting and remove the warning
[2026-01-15T02:27:54.045+0000] {docker.py:429} INFO - 2026-01-15 02:27:54,044 - SparkProcess - INFO - Starte Processing Job...
[2026-01-15T02:27:55.589+0000] {docker.py:429} INFO - Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[2026-01-15T02:27:55.740+0000] {docker.py:429} INFO - 26/01/15 02:27:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2026-01-15T02:27:56.784+0000] {docker.py:429} INFO - 2026-01-15 02:27:56,784 - SparkProcess - INFO - Verarbeite: yellow_tripdata_2015-01.csv
[2026-01-15T02:27:57.416+0000] {docker.py:429} INFO - 26/01/15 02:27:57 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
[2026-01-15T02:28:00.669+0000] {docker.py:429} INFO - [Stage 0:>                                                         (0 + 0) / 15]
[2026-01-15T02:28:15.055+0000] {docker.py:429} INFO - 26/01/15 02:28:15 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2026-01-15T02:28:30.055+0000] {docker.py:429} INFO - 26/01/15 02:28:30 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2026-01-15T02:28:45.056+0000] {docker.py:429} INFO - 26/01/15 02:28:45 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2026-01-15T02:29:00.055+0000] {docker.py:429} INFO - 26/01/15 02:29:00 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2026-01-15T02:29:00.707+0000] {docker.py:429} INFO - [Stage 0:>                                                         (0 + 0) / 15]
[2026-01-15T02:29:15.056+0000] {docker.py:429} INFO - 26/01/15 02:29:15 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2026-01-15T02:29:30.055+0000] {docker.py:429} INFO - 26/01/15 02:29:30 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2026-01-15T02:29:45.055+0000] {docker.py:429} INFO - 26/01/15 02:29:45 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2026-01-15T02:30:00.055+0000] {docker.py:429} INFO - 26/01/15 02:30:00 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2026-01-15T02:30:00.744+0000] {docker.py:429} INFO - [Stage 0:>                                                         (0 + 0) / 15]
[2026-01-15T02:30:15.055+0000] {docker.py:429} INFO - 26/01/15 02:30:15 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2026-01-15T02:30:30.055+0000] {docker.py:429} INFO - 26/01/15 02:30:30 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2026-01-15T02:30:45.055+0000] {docker.py:429} INFO - 26/01/15 02:30:45 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2026-01-15T02:31:00.055+0000] {docker.py:429} INFO - 26/01/15 02:31:00 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2026-01-15T02:31:00.770+0000] {docker.py:429} INFO - [Stage 0:>                                                         (0 + 0) / 15]
[2026-01-15T02:31:15.055+0000] {docker.py:429} INFO - 26/01/15 02:31:15 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2026-01-15T02:31:30.054+0000] {docker.py:429} INFO - 26/01/15 02:31:30 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2026-01-15T02:31:45.055+0000] {docker.py:429} INFO - 26/01/15 02:31:45 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2026-01-15T02:32:00.055+0000] {docker.py:429} INFO - 26/01/15 02:32:00 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2026-01-15T02:32:00.806+0000] {docker.py:429} INFO - [Stage 0:>                                                         (0 + 0) / 15]
[2026-01-15T02:32:15.055+0000] {docker.py:429} INFO - 26/01/15 02:32:15 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2026-01-15T02:32:15.416+0000] {local_task_job_runner.py:302} WARNING - State of this instance has been externally set to skipped. Terminating instance.
[2026-01-15T02:32:15.418+0000] {process_utils.py:131} INFO - Sending 15 to group 212. PIDs of all processes in the group: [212]
[2026-01-15T02:32:15.418+0000] {process_utils.py:86} INFO - Sending the signal 15 to group 212
[2026-01-15T02:32:15.418+0000] {taskinstance.py:2450} ERROR - Received SIGTERM. Terminating subprocesses.
[2026-01-15T02:32:15.418+0000] {docker.py:521} INFO - Stopping docker container
[2026-01-15T02:32:25.818+0000] {process_utils.py:79} INFO - Process psutil.Process(pid=212, status='terminated', exitcode=0, started='02:27:52') (212) terminated with exit code 0
