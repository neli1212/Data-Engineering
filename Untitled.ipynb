{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2c36e51-8cfb-48a0-9593-6fc1607c2c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/29 15:32:13 WARN Utils: Your hostname, neliN resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "26/01/29 15:32:13 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/neli/.local/share/mamba/envs/data-engineering/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/neli/.ivy2/cache\n",
      "The jars for the packages stored in: /home/neli/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "com.amazonaws#aws-java-sdk-bundle added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-31180882-5141-453a-acf7-dec9edae1339;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.262 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.4/hadoop-aws-3.3.4.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-aws;3.3.4!hadoop-aws.jar (64ms)\n",
      "downloading https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.262/aws-java-sdk-bundle-1.12.262.jar ...\n",
      "\t[SUCCESSFUL ] com.amazonaws#aws-java-sdk-bundle;1.12.262!aws-java-sdk-bundle.jar (8441ms)\n",
      "downloading https://repo1.maven.org/maven2/org/wildfly/openssl/wildfly-openssl/1.0.7.Final/wildfly-openssl-1.0.7.Final.jar ...\n",
      "\t[SUCCESSFUL ] org.wildfly.openssl#wildfly-openssl;1.0.7.Final!wildfly-openssl.jar (48ms)\n",
      ":: resolution report :: resolve 2240ms :: artifacts dl 8556ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   3   |   3   |   0   ||   3   |   3   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-31180882-5141-453a-acf7-dec9edae1339\n",
      "\tconfs: [default]\n",
      "\t3 artifacts copied, 0 already retrieved (275421kB/429ms)\n",
      "26/01/29 15:32:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/29 15:32:29 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows found: 47248845\n",
      "root\n",
      " |-- vendorid: string (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- tpep_dropoff_datetime: string (nullable = true)\n",
      " |-- passenger_count: string (nullable = true)\n",
      " |-- trip_distance: string (nullable = true)\n",
      " |-- pickup_longitude: string (nullable = true)\n",
      " |-- pickup_latitude: string (nullable = true)\n",
      " |-- ratecodeid: string (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- dropoff_longitude: string (nullable = true)\n",
      " |-- dropoff_latitude: string (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- fare_amount: string (nullable = true)\n",
      " |-- extra: string (nullable = true)\n",
      " |-- mta_tax: string (nullable = true)\n",
      " |-- tip_amount: string (nullable = true)\n",
      " |-- tolls_amount: string (nullable = true)\n",
      " |-- improvement_surcharge: string (nullable = true)\n",
      " |-- total_amount: string (nullable = true)\n",
      " |-- processed_at: timestamp (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      "\n",
      "+--------+--------------------+---------------------+---------------+-------------+-------------------+------------------+----------+------------------+-------------------+------------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+----+-----+\n",
      "|vendorid|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|   pickup_longitude|   pickup_latitude|ratecodeid|store_and_fwd_flag|  dropoff_longitude|  dropoff_latitude|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|        processed_at|year|month|\n",
      "+--------+--------------------+---------------------+---------------+-------------+-------------------+------------------+----------+------------------+-------------------+------------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+----+-----+\n",
      "|       2| 2015-01-15 20:05:39|  2015-01-15 19:23:42|              1|         1.59|   -73.993896484375|40.750110626220703|         1|                 N|-73.974784851074219|40.750617980957031|           1|         12|    1|    0.5|      3.25|           0|                  0.3|       17.05|2026-01-29 15:17:...|2015|    1|\n",
      "|       2| 2015-01-03 02:16:51|  2015-01-03 01:29:40|              2|         3.59|-74.013946533203125|40.713901519775391|         1|                 N|-73.983650207519531|40.735031127929688|           1|         13|  0.5|    0.5|       2.7|           0|                  0.3|          17|2026-01-29 15:17:...|2015|    1|\n",
      "|       1| 2015-01-10 21:33:38|  2015-01-10 20:53:28|              1|         3.30| -74.00164794921875|  40.7242431640625|         1|                 N|-73.994415283203125|40.759109497070313|           1|       14.5|  0.5|    0.5|         2|           0|                  0.3|        17.8|2026-01-29 15:17:...|2015|    1|\n",
      "|       2| 2015-01-03 02:16:51|  2015-01-03 01:29:30|              1|         4.44| -73.97015380859375|40.756439208984375|         1|                 N|-73.964569091796875|40.804161071777344|           1|       14.5|  0.5|    0.5|         2|           0|                  0.3|        17.8|2026-01-29 15:17:...|2015|    1|\n",
      "|       1| 2015-01-10 21:33:38|  2015-01-10 20:43:41|              1|         1.80|-73.963340759277344|40.802787780761719|         1|                 N|-73.951820373535156|40.824413299560547|           2|        9.5|  0.5|    0.5|         0|           0|                  0.3|        10.8|2026-01-29 15:17:...|2015|    1|\n",
      "|       2| 2015-01-03 02:16:51|  2015-01-03 01:24:45|              1|         2.94| -73.99383544921875|  40.7210693359375|         1|                 N|-73.943916320800781| 40.71197509765625|           1|       10.5|  0.5|    0.5|       2.2|           0|                  0.3|          14|2026-01-29 15:17:...|2015|    1|\n",
      "|       1| 2015-01-10 21:33:39|  2015-01-10 20:35:31|              1|          .50|-74.009086608886719|40.713817596435547|         1|                 N|-74.004325866699219|40.719985961914063|           2|        3.5|  0.5|    0.5|         0|           0|                  0.3|         4.8|2026-01-29 15:17:...|2015|    1|\n",
      "|       2| 2015-01-03 02:16:51|  2015-01-03 01:24:50|              4|         1.38|-73.987831115722656|40.738216400146484|         1|                 N|-74.006309509277344|40.733360290527344|           2|        7.5|  0.5|    0.5|         0|           0|                  0.3|         8.8|2026-01-29 15:17:...|2015|    1|\n",
      "|       1| 2015-01-10 21:33:39|  2015-01-10 20:52:58|              1|         3.00|-73.971176147460938|40.762428283691406|         1|                 N|-74.004180908203125|40.742652893066406|           2|         15|  0.5|    0.5|         0|           0|                  0.3|        16.3|2026-01-29 15:17:...|2015|    1|\n",
      "|       1| 2015-01-03 02:16:52|  2015-01-03 01:23:26|              1|         1.30|-74.005325317382812|40.719547271728516|         1|                 N|-74.003746032714844|40.736660003662109|           1|        6.5|  0.5|    0.5|       1.5|           0|                    0|         9.3|2026-01-29 15:17:...|2015|    1|\n",
      "+--------+--------------------+---------------------+---------------+-------------+-------------------+------------------+----------+------------------+-------------------+------------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+----+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+----+-----+--------+\n",
      "|year|month|   count|\n",
      "+----+-----+--------+\n",
      "|2015|    1|12748986|\n",
      "|2016|    1|10906858|\n",
      "|2016|    2|11382049|\n",
      "|2016|    3|12210952|\n",
      "+----+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "MINIO_ENDPOINT = \"http://localhost:9000\"\n",
    "ACCESS_KEY = \"minioadmin\"\n",
    "SECRET_KEY = \"minioadmin\"\n",
    "MASTER_URL = \"local[*]\" \n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MinioDataCheck\") \\\n",
    "    .master(MASTER_URL) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", MINIO_ENDPOINT) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", ACCESS_KEY) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", SECRET_KEY) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "df = spark.read.parquet(TARGET_PATH)\n",
    "\n",
    "# 3. INSPECTION\n",
    "print(f\"Total rows found: {df.count()}\")\n",
    "df.printSchema()\n",
    "\n",
    "# Show the top 10 rows\n",
    "df.show(10)\n",
    "\n",
    "# Check the distribution across your partitions\n",
    "df.groupBy(\"year\", \"month\").count().orderBy(\"year\", \"month\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58c57f55-8039-4255-8c73-9efb469fba6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 22 columns):\n",
      " #   Column                 Non-Null Count  Dtype         \n",
      "---  ------                 --------------  -----         \n",
      " 0   vendorid               10000 non-null  object        \n",
      " 1   tpep_pickup_datetime   10000 non-null  datetime64[ns]\n",
      " 2   tpep_dropoff_datetime  10000 non-null  object        \n",
      " 3   passenger_count        10000 non-null  object        \n",
      " 4   trip_distance          10000 non-null  object        \n",
      " 5   pickup_longitude       10000 non-null  object        \n",
      " 6   pickup_latitude        10000 non-null  object        \n",
      " 7   ratecodeid             10000 non-null  object        \n",
      " 8   store_and_fwd_flag     10000 non-null  object        \n",
      " 9   dropoff_longitude      10000 non-null  object        \n",
      " 10  dropoff_latitude       10000 non-null  object        \n",
      " 11  payment_type           10000 non-null  object        \n",
      " 12  fare_amount            10000 non-null  object        \n",
      " 13  extra                  10000 non-null  object        \n",
      " 14  mta_tax                10000 non-null  object        \n",
      " 15  tip_amount             10000 non-null  object        \n",
      " 16  tolls_amount           10000 non-null  object        \n",
      " 17  improvement_surcharge  10000 non-null  object        \n",
      " 18  total_amount           10000 non-null  object        \n",
      " 19  processed_at           10000 non-null  datetime64[ns]\n",
      " 20  year                   10000 non-null  int32         \n",
      " 21  month                  10000 non-null  int32         \n",
      "dtypes: datetime64[ns](2), int32(2), object(18)\n",
      "memory usage: 1.6+ MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vendorid</th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>ratecodeid</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>...</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>processed_at</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2015-01-15 20:05:39</td>\n",
       "      <td>2015-01-15 19:23:42</td>\n",
       "      <td>1</td>\n",
       "      <td>1.59</td>\n",
       "      <td>-73.993896484375</td>\n",
       "      <td>40.750110626220703</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>-73.974784851074219</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.25</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>17.05</td>\n",
       "      <td>2026-01-29 15:17:40.560717</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2015-01-03 02:16:51</td>\n",
       "      <td>2015-01-03 01:29:40</td>\n",
       "      <td>2</td>\n",
       "      <td>3.59</td>\n",
       "      <td>-74.013946533203125</td>\n",
       "      <td>40.713901519775391</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>-73.983650207519531</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>17</td>\n",
       "      <td>2026-01-29 15:17:40.560717</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-10 21:33:38</td>\n",
       "      <td>2015-01-10 20:53:28</td>\n",
       "      <td>1</td>\n",
       "      <td>3.30</td>\n",
       "      <td>-74.00164794921875</td>\n",
       "      <td>40.7242431640625</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>-73.994415283203125</td>\n",
       "      <td>...</td>\n",
       "      <td>14.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>17.8</td>\n",
       "      <td>2026-01-29 15:17:40.560717</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2015-01-03 02:16:51</td>\n",
       "      <td>2015-01-03 01:29:30</td>\n",
       "      <td>1</td>\n",
       "      <td>4.44</td>\n",
       "      <td>-73.97015380859375</td>\n",
       "      <td>40.756439208984375</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>-73.964569091796875</td>\n",
       "      <td>...</td>\n",
       "      <td>14.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>17.8</td>\n",
       "      <td>2026-01-29 15:17:40.560717</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-10 21:33:38</td>\n",
       "      <td>2015-01-10 20:43:41</td>\n",
       "      <td>1</td>\n",
       "      <td>1.80</td>\n",
       "      <td>-73.963340759277344</td>\n",
       "      <td>40.802787780761719</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>-73.951820373535156</td>\n",
       "      <td>...</td>\n",
       "      <td>9.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>10.8</td>\n",
       "      <td>2026-01-29 15:17:40.560717</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  vendorid tpep_pickup_datetime tpep_dropoff_datetime passenger_count  \\\n",
       "0        2  2015-01-15 20:05:39   2015-01-15 19:23:42               1   \n",
       "1        2  2015-01-03 02:16:51   2015-01-03 01:29:40               2   \n",
       "2        1  2015-01-10 21:33:38   2015-01-10 20:53:28               1   \n",
       "3        2  2015-01-03 02:16:51   2015-01-03 01:29:30               1   \n",
       "4        1  2015-01-10 21:33:38   2015-01-10 20:43:41               1   \n",
       "\n",
       "  trip_distance     pickup_longitude     pickup_latitude ratecodeid  \\\n",
       "0          1.59     -73.993896484375  40.750110626220703          1   \n",
       "1          3.59  -74.013946533203125  40.713901519775391          1   \n",
       "2          3.30   -74.00164794921875    40.7242431640625          1   \n",
       "3          4.44   -73.97015380859375  40.756439208984375          1   \n",
       "4          1.80  -73.963340759277344  40.802787780761719          1   \n",
       "\n",
       "  store_and_fwd_flag    dropoff_longitude  ... fare_amount extra mta_tax  \\\n",
       "0                  N  -73.974784851074219  ...          12     1     0.5   \n",
       "1                  N  -73.983650207519531  ...          13   0.5     0.5   \n",
       "2                  N  -73.994415283203125  ...        14.5   0.5     0.5   \n",
       "3                  N  -73.964569091796875  ...        14.5   0.5     0.5   \n",
       "4                  N  -73.951820373535156  ...         9.5   0.5     0.5   \n",
       "\n",
       "  tip_amount tolls_amount improvement_surcharge total_amount  \\\n",
       "0       3.25            0                   0.3        17.05   \n",
       "1        2.7            0                   0.3           17   \n",
       "2          2            0                   0.3         17.8   \n",
       "3          2            0                   0.3         17.8   \n",
       "4          0            0                   0.3         10.8   \n",
       "\n",
       "                processed_at  year month  \n",
       "0 2026-01-29 15:17:40.560717  2015     1  \n",
       "1 2026-01-29 15:17:40.560717  2015     1  \n",
       "2 2026-01-29 15:17:40.560717  2015     1  \n",
       "3 2026-01-29 15:17:40.560717  2015     1  \n",
       "4 2026-01-29 15:17:40.560717  2015     1  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a random 0.1% sample or just the first 10,000 rows\n",
    "pandas_df = df.limit(10000).toPandas()\n",
    "\n",
    "# Now you can use all your favorite Pandas functions\n",
    "print(pandas_df.info())\n",
    "pandas_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "561fb9ce-7b70-46fc-bd20-5fd2a00c108a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- RAW TIMESTAMP RANGE ---\n",
      "      earliest_record       latest_record\n",
      "0 2015-01-01 01:00:00 2016-04-01 01:59:59\n",
      "\n",
      "--- ACTUAL DATA DISTRIBUTION ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   true_year  true_month     count\n",
      "0       2015           1  12719130\n",
      "1       2015           2     29856\n",
      "2       2016           1  10897600\n",
      "3       2016           2  11379693\n",
      "4       2016           3  12176050\n",
      "5       2016           4     46516\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# 1. Find the absolute Minimum and Maximum dates in the actual timestamp column\n",
    "date_range = df.select(\n",
    "    F.min(\"tpep_pickup_datetime\").alias(\"earliest_record\"),\n",
    "    F.max(\"tpep_pickup_datetime\").alias(\"latest_record\")\n",
    ").toPandas()\n",
    "\n",
    "print(\"--- RAW TIMESTAMP RANGE ---\")\n",
    "print(date_range)\n",
    "\n",
    "# 2. Extract the Year and Month DIRECTLY from the timestamp column \n",
    "# (Ignoring your folder names/partitions)\n",
    "true_distribution = df.withColumn(\"true_year\", F.year(\"tpep_pickup_datetime\")) \\\n",
    "                      .withColumn(\"true_month\", F.month(\"tpep_pickup_datetime\")) \\\n",
    "                      .groupBy(\"true_year\", \"true_month\") \\\n",
    "                      .count() \\\n",
    "                      .orderBy(\"true_year\", \"true_month\")\n",
    "\n",
    "print(\"\\n--- ACTUAL DATA DISTRIBUTION ---\")\n",
    "print(true_distribution.toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4c78645-4812-4f44-be35-fe11734ec3ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 's3fs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01ms3fs\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# 1. Setup MinIO Connection (No Spark needed)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m endpoint_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://localhost:9000\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 's3fs'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import s3fs\n",
    "\n",
    "# 1. Setup MinIO Connection (No Spark needed)\n",
    "endpoint_url = \"http://localhost:9000\"\n",
    "key = \"minioadmin\"\n",
    "secret = \"minioadmin\"\n",
    "\n",
    "# Create the filesystem object\n",
    "fs = s3fs.S3FileSystem(\n",
    "    client_kwargs={'endpoint_url': endpoint_url},\n",
    "    key=key,\n",
    "    secret=secret\n",
    ")\n",
    "\n",
    "# 2. Define the paths to the \"Missing\" data in your processed bucket\n",
    "# We are going to try and read the files directly from the folders\n",
    "paths = [\n",
    "    \"s3://processed-data/nyc_taxi/year=2015/month=2/\",\n",
    "    \"s3://processed-data/nyc_taxi/year=2016/month=4/\"\n",
    "]\n",
    "\n",
    "print(\"--- DIRECT PANDAS READ ---\")\n",
    "\n",
    "for path in paths:\n",
    "    try:\n",
    "        # We use 'fs' to list the files and read the first one found\n",
    "        files = fs.ls(path)\n",
    "        if not files:\n",
    "            print(f\"Empty: No files found in {path}\")\n",
    "            continue\n",
    "            \n",
    "        # Read just the first file in that folder into a Pandas DF\n",
    "        # We only take 10 rows (nrows doesn't work for parquet, so we head(10))\n",
    "        df = pd.read_parquet(f\"s3://{files[0]}\", filesystem=fs)\n",
    "        \n",
    "        print(f\"\\nFolder: {path}\")\n",
    "        print(df[['tpep_pickup_datetime', 'vendorid', 'total_amount']].head(10))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bc7d4f-b9dc-4a28-bedb-4f4713e8251b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
